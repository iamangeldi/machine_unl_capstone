{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxRNisAfXSvl"
      },
      "source": [
        "This notebook directly works with implementing and producing results, of chosen metrics, correlated with this project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L56Z0UnGXSvm"
      },
      "source": [
        "## Metric #1: CLIP-Based Style Similarity\n",
        "\n",
        "### Given\n",
        "\n",
        "* A fixed evaluation prompt set divided into:\n",
        "  * Positive prompts (intended to elicit the tattoo style)\n",
        "  * Negative prompts (intended to avoid the tattoo style)\n",
        "\n",
        "* A fixed set of random seeds\n",
        "\n",
        "* Three model configurations:\n",
        "  * Base model\n",
        "  * Base + Original LoRA (fused)\n",
        "  * Base + Recreated LoRA (fused)\n",
        "\n",
        "* A held-out dataset of tattoo-style reference images not used during LoRA training\n",
        "\n",
        "All generation parameters (sampler, number of steps, guidance scale, resolution) are held constant across models.\n",
        "\n",
        "---\n",
        "\n",
        "### Procedure\n",
        "\n",
        "1. For each model, generate images using the same prompts and seeds.\n",
        "\n",
        "2. Compute CLIP image embeddings for:\n",
        "   * All generated images\n",
        "   * All held-out reference tattoo images\n",
        "\n",
        "3. For each generated image:\n",
        "   * Compute cosine similarity to the reference set\n",
        "   * Use top-k average similarity (or nearest-neighbor similarity)\n",
        "\n",
        "4. Aggregate similarity scores:\n",
        "   * Mean and standard deviation across all images\n",
        "   * Separate averages for positive prompts\n",
        "   * Separate averages for negative prompts\n",
        "\n",
        "5. Optionally compute a normalized recovery score:\n",
        "   * Recovery = (Score_recreated − Score_base) / (Score_original − Score_base)\n",
        "\n",
        "---\n",
        "\n",
        "### Output\n",
        "\n",
        "For each model, report:\n",
        "\n",
        "* Mean CLIP similarity to held-out tattoo references\n",
        "* Standard deviation\n",
        "* Separate similarity scores for positive and negative prompts\n",
        "* Optional: normalized recovery score\n",
        "\n",
        "---\n",
        "\n",
        "### Meaning\n",
        "\n",
        "This metric evaluates how closely generated images align with unseen tattoo-style works in CLIP embedding space.\n",
        "\n",
        "If the recreated model achieves similarity scores close to the original LoRA model—and substantially higher than the base model—this indicates successful recovery of stylistic behavior.\n",
        "\n",
        "A strong reconstruction should:\n",
        "\n",
        "* Produce high similarity for positive prompts\n",
        "* Maintain low similarity for negative prompts\n",
        "* Closely match the original LoRA’s embedding-level behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKzoF2ApXSvm"
      },
      "source": [
        "### Load base, base+fused, base+recovered models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Y5t-zwqqXSvn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline, AutoencoderKL\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=dtype)\n",
        "\n",
        "def load_pipe(repo_id: str): # helper function to load the models modularly\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        repo_id,\n",
        "        vae=vae,\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\" if dtype == torch.float16 else None,\n",
        "        use_safetensors=True,\n",
        "    ).to(device)\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "    return pipe\n",
        "\n",
        "pipe_base  = load_pipe(\"stabilityai/stable-diffusion-xl-base-1.0\") # base\n",
        "pipe_fused = load_pipe(\"francuzama/sdxl-fused\") # original+fused\n",
        "pipe_recovered = load_pipe(\"francuzama/sdxl-recovered\") # original+recovered\n",
        "\n",
        "positive_prompt_set = [\"a tattoo in the style of JASON, a roaring tiger, bold black ink, yellow highlights, clean linework, high contrast\", \"a tattoo in the style of JASON, a dagger wrapped in roses, bold black ink, yellow highlights, sharp outlines, flat shadin\", \"a tattoo in the style of JASON, a mechanical owl with spread wings, bold black ink, yellow highlights, strong outlines, graphic style\"]\n",
        "negative_prompt_set = [\"a roaring tiger, realistic wildlife photograph, natural lighting, DSLR photo\", \"a dagger wrapped in roses, watercolor illustration, soft edges, pastel colors\", \"a mechanical owl with spread wings, 3D render, cinematic lighting, ultra detailed\"]\n",
        "seed_set = [1234, 667, 3067]\n",
        "\n",
        "def gen(pipe, outname):\n",
        "    g = torch.Generator(device=device).manual_seed(seed)\n",
        "    img = pipe(prompt, num_inference_steps=30, guidance_scale=7.0, generator=g).images[0]\n",
        "    img.save(outname)\n",
        "\n",
        "gen(pipe_base,  \"base.png\")\n",
        "gen(pipe_fused, \"fused.png\")\n",
        "\n",
        "print(\"Saved base.png and fused.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT1JOKUVXSvn"
      },
      "source": [
        "## Metric #2: Weight-Space Reconstruction Similarity\n",
        "\n",
        "### Given\n",
        "\n",
        "* Three model checkpoints:\n",
        "  * Base model  \n",
        "  * Base + Original LoRA (fused)  \n",
        "  * Base + Recreated LoRA (fused)\n",
        "\n",
        "* Access to full model weights (e.g., UNet and relevant submodules)\n",
        "\n",
        "* The ability to compute weight differences relative to the base model\n",
        "\n",
        "All models must share the same architecture and base initialization.\n",
        "\n",
        "---\n",
        "\n",
        "### Procedure\n",
        "\n",
        "1. Compute weight deltas relative to the base model:\n",
        "\n",
        "   * Delta (Original) = Fused Original − Base  \n",
        "   * Delta (Recreated) = Fused Recreated − Base  \n",
        "\n",
        "2. For each corresponding layer:\n",
        "\n",
        "   * Flatten weight tensors  \n",
        "   * Compute L2 distance between delta tensors  \n",
        "   * Optionally compute cosine similarity between flattened tensors  \n",
        "\n",
        "3. Aggregate across layers:\n",
        "\n",
        "   * Compute overall L2 distance  \n",
        "   * Compute relative L2 error:\n",
        "     * Relative Error = ||Delta_original − Delta_recreated||₂ / ||Delta_original||₂  \n",
        "\n",
        "4. Optionally analyze similarity at different levels:\n",
        "\n",
        "   * Per-layer similarity (attention blocks, MLP layers, etc.)  \n",
        "   * Global similarity across entire UNet  \n",
        "\n",
        "---\n",
        "\n",
        "### Output\n",
        "\n",
        "For the reconstructed model, report:\n",
        "\n",
        "* Overall L2 distance between weight deltas  \n",
        "* Relative L2 error  \n",
        "* Optional: cosine similarity between deltas  \n",
        "* Optional: per-layer similarity statistics (mean, worst layer, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### Meaning\n",
        "\n",
        "This metric evaluates how closely the attacker’s reconstructed model matches the original fused model in parameter space.\n",
        "\n",
        "If the recreated delta closely matches the original delta (low relative L2 error, high cosine similarity), this indicates successful recovery of the learned style at the parameter level.\n",
        "\n",
        "A strong reconstruction should:\n",
        "\n",
        "* Produce small global L2 difference  \n",
        "* Maintain high cosine alignment between weight deltas  \n",
        "* Show consistent similarity across major architectural components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zz_lBbXSvn"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}